# conf/model/token_count_predictor.yaml
_target_: models.neural_token_count_predictor.NeuralTokenCountPredictor

# device to run on
device: cuda      

# --- Backbone/feature extractor: can be either a CNN alone or VAE encoder & CNN or Resnet---
# one of: sd-vae | resnet | rgb
backbone_type: "clip"

# used when backbone == "sd-vae"
hf_hub_path: "stabilityai/sd-vae-ft-mse"
freeze_vae: true

# used when backbone == "resnet"
resnet_name: "resnet18"         # any torchvision name (resnet18/34/50)
resnet_layer: "layer1"          # "layer2" (128x32x32) or "layer3" (256x16x16)
resnet_feature_dim: 64  # feature dimension of the extracted layer 64 for layer1, 128 for layer2, 256 for layer3
resnet_pretrained: true

# used when backbone == "clip"
clip_model_name: "openai/clip-vit-base-patch32"  # could also be RN50, RN101, etc.
clip_feature_dim: 512            # depends on model (ViT-B/32 = 512, ViT-L/14 = 768, RN50 = 1024)
clip_pretrained: true            # always true for CLIP, since we rely on pretrained


# --- CNN trunk (for RGB path) ---
# We define per-layer channels and strides. Stride=2 downsamples spatially.
# Example below (for 256x256 images → latents ~ [B,4,32,32]):
#   C0:  3 →  32 @ 128x128   (stride 2)
#   C1: 32 →  64 @ 64x64   (stride 2)
#   C2: 64 → 128 @  32x32   (stride 2)
conv_channels: [] #[32, 64, 128]       # output channels for each conv block. empty for clip
conv_strides:  [] #[2,   2,   2]         # stride per conv block (same length)
norm: group                                # group | batch | none
num_groups: 8                               # used if norm=group
activation: silu                      # relu | gelu | silu | tanh
dropout2d: 0.10                            # dropout after conv blocks (2D)

# Optional pooling after the conv stack:
#   none  → keep spatial dims, then flatten
#   gap   → global average pool to [B, C], skip flatten
post_conv_pool: none                       # none | gap


# --- Conditioning vector (tolerated loss etc.) ---
use_loss_condition: false
loss_dim: 1                               # size of conditioning vector
loss_proj_dim: 164                        # embed cond → this many dims


# --- Task ---
head: "regression"               # classification | regression
num_classes: 256                     # for classification: predict counts in {1..256}. for regression: predicts only 1 class


# --- Fully-connected head (regression) ---
# Applied after (flatten OR GAP) and concat with loss embedding.
fc_hidden: [128, 64]                      


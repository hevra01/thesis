defaults:
  - /model: token_predictor_heuristic_baseline
  - /dataset: imageNet

optimizer:
  _target_: torch.optim.Adam
  lr: 0.001

training:
  num_epochs: 800
  loss_training:
    _target_: reconstruction_loss.GaussianCrossEntropyLoss
  loss_analysis:
    _target_: reconstruction_loss.MAELoss

# ---------------------------------------------
# Reconstruction dataset and feature toggles
# ---------------------------------------------
reconstruction_dataset:
  reconstruction_data_path: "data/datasets/reconstruction_loss_imgnet_train/reconstruction_errors_0000_0070.json"
  batch_size: 120
  shuffle: true
  reconstruction_loss: "vgg_error"   # field name in reconstruction_data to use as the loss feature

  # Feature toggles: enable/disable auxiliary features to concatenate with the reconstruction loss
  use_edge_ratio: false
  edge_ratio_path: "data/datasets/imageNet_edge_ratios/train_imageNet_edge_ratios.json"

  use_lid: true
  lid_path: "data/datasets/imageNet_LID_values/training_set/all_lids.json"          # path to JSON (dict or list indexed by image_id)

  use_local_density: false
  local_density_path: "data/datasets/density/train_imageNet_local_density.json"  # path to JSON (dict or list indexed by image_id)

  # Optional filtering of samples by an error metric
  # filter:
  #   key: vgg_error
  #   min: 2.83040
  #   max: 2.83640

# ---------------------------------------------
# Model overrides
# Ensure in_dim equals 1 (recon loss) + sum(enabled auxiliary features)
# With the defaults above (edge_ratio enabled only), in_dim = 2.
model:
  mode: classification            # training uses GaussianCrossEntropyLoss
  in_dim: 2                       # adjust to 1 + (use_edge_ratio) + (use_lid) + (use_local_density)
  num_classes: 256
  # -----------------------------
  # Preprocessing overrides (applied inside the model)
  # We recommend signed log1p for heterogeneous ranges (e.g., LID can be negative and huge)
  # Per-feature transform masks (length must equal in_dim). With current toggles, order is [mse_error, lid].
  # Example: do not transform MSE (already small), use signed log for LID.
  use_signed_log1p: [false, false]
  standardize: true              # require per-feature mean/std below (length must equal in_dim)
  # IMPORTANT: Order of features is exactly how the training script builds the tensor:
  #   [ reconstruction_loss, (edge_ratio if enabled), (lid if enabled), (local_density if enabled) ]
  # With the toggles above (use_lid: true, others false), the order is [mse_error, lid]
  # Replace the placeholders with your precomputed stats after applying the same log policy.
  
  # if applying log:
  # Mean VGG Error (signed log1p): 1.3205, Std VGG Error (signed log1p): 0.2188
  # Mean MSE Error (signed log1p): 0.1973, Std MSE Error (signed log1p): 0.0612
  
  # if applying no log:
  # Mean VGG Error before log: 2.8324, Std VGG Error before log: 0.7998
  # Mean MSE Error before log: 0.2203, Std MSE Error before log: 0.0754

  # Mean LID (signed log1p): -5541565.5392, Std LID (signed log1p): 708356.6451

  standardize_mean: [2.8324, -5541565.5392]   # [mean_mse_error_after_log, mean_lid_after_log]
  standardize_std:  [0.7998, 708356.6451]   # [std_mse_error_after_log,  std_lid_after_log]

checkpoint_path: "neural_baseline/checkpoint/heuristic_smallDatasdds_LID_mse_sdntsdszewest.pt"

dataset:
  split: "train"

experiment_name: "heuristic_AlllData_LID_vgg_no_log_high_lr"
device: "cuda"


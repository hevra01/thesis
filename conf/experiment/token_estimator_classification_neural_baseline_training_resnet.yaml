defaults:
  - /model: resnet_fine_tune
  - /dataset: imageNet

dataset_root: /scratch/inf0/user/mparcham/ILSVRC2012

dataset_train:
  _target_: data.utils.dataloaders.get_imagenet_dataloader
  root: ${dataset_root}
  split: train
  train_or_eval: "train"

dataset_val:
  _target_: data.utils.dataloaders.get_imagenet_dataloader
  root: ${dataset_root}
  split: val_categorized
  train_or_eval: "eval"

optimizer:
  _target_: torch.optim.Adam
  # global default lr (used if group-specific not provided)
  lr: 0.001

optimizer_lr:
  # optional per-group overrides consumed in training.py
  lr_backbone: 0.00004
  lr_head: 0.0004

training:
    num_epochs: 100
    loss_training: 
      _target_: reconstruction_loss.GaussianCrossEntropyLoss
      sigma: 0.2

# this is for the main dataset that has the mse_errors, vgg_errors for all the images for different
# values of k_values and the  bpp.
reconstruction_dataset:
  reconstruction_train_data_path: "data/datasets/imagenet_reconstruction_losses/train/all_losses.json"
  reconstruction_test_data_path: "data/datasets/imagenet_reconstruction_losses/test/all_losses.json"
  batch_size: 180 # this is the batch size for the reconstruction dataset on A40, 4 GPU, 180 batch size is possible
  shuffle: true
  reconstruction_loss: "LPIPS"  # e.g., "mse_error" or "vgg_error"
  filter_key: LPIPS
  min_error: 0.02079272
  max_error: 0.12662399

experiment_name: "classification_resnet"
project_name: neural_baselines_resnet_classification
group_name: resnet_baseline_experiments
checkpoint_path: "neural_baseline/checkpoint/LPIPS/"

model:
  freeze_backbone: false
  use_condition:  true

defaults:
  - /model: loss_to_tokenCount_model
  - /dataset: imageNet


optimizer:
  _target_: torch.optim.Adam
  lr: 0.0001

training:
    num_epochs: 200
    loss_training: 
      _target_: reconstruction_loss.GaussianCrossEntropyLoss
    loss_analysis: 
      _target_: reconstruction_loss.MAELoss

# this is for the main dataset that has the mse_errors, vgg_errors for all the images for different
# values of k_values and the  bpp.
reconstruction_dataset:
  reconstruction_data_path: "/BS/data_mani_compress/work/thesis/thesis/data/datasets/imagenet_reconstruction_losses/val_categorized/all_APG_on.json"
  batch_size: 120 # this is the batch size for the reconstruction dataset
  shuffle: true
  # Optional: filter samples by an error range to simplify training.
  # Set 'key' to the metric to filter on (e.g., 'vgg_error' or 'mse_error').
  # Set 'min'/'max' to numeric bounds (inclusive). Leave as null to disable.
  # filter:
  #   key: vgg_error   # e.g., vgg_error | mse_error
  #   min: 2.83040   # e.g., 0.30
  #   max: 2.83640   # e.g., 0.35
  reconstruction_loss: "LPIPS"  # e.g., "mse_error" or "vgg_error"

checkpoint_path: "neural_baseline/checkpoint/loss_tokend_lpips.pt"

# different experiments might use different splits of the data
dataset:
  split: "val_categorized"

experiment_name: "token_estimator_given_loss_LPIPS"
device: "cuda"

